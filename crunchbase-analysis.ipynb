{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TO-DO:\" data-toc-modified-id=\"TO-DO:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TO DO:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Think-about-the-target-variable-and-other-possible-ones\" data-toc-modified-id=\"Think-about-the-target-variable-and-other-possible-ones-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Think about the target variable and other possible ones</a></span></li><li><span><a href=\"#Add-more-charts-instead-of-value_counts(),-and-a-function-to-replace-that-and-add-on\" data-toc-modified-id=\"Add-more-charts-instead-of-value_counts(),-and-a-function-to-replace-that-and-add-on-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Add more charts instead of value_counts(), and a function to replace that and add on</a></span></li><li><span><a href=\"#More-feature-engineering---mostly-around-outliers/the---in-the-funding-total?\" data-toc-modified-id=\"More-feature-engineering---mostly-around-outliers/the---in-the-funding-total?-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>More feature engineering - mostly around outliers/the - in the funding total?</a></span></li><li><span><a href=\"#Summary-statistics,-probably-in-that-function-above,-investigate-the-data-deeper\" data-toc-modified-id=\"Summary-statistics,-probably-in-that-function-above,-investigate-the-data-deeper-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Summary statistics, probably in that function above, investigate the data deeper</a></span></li><li><span><a href=\"#Research-how-the-funding-process-works(domain-knowledge)\" data-toc-modified-id=\"Research-how-the-funding-process-works(domain-knowledge)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Research how the funding process works(domain knowledge)</a></span></li><li><span><a href=\"#Work-on-tuning-the-clustering-algorithms-and-using-them-in-conjunction\" data-toc-modified-id=\"Work-on-tuning-the-clustering-algorithms-and-using-them-in-conjunction-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Work on tuning the clustering algorithms and using them in conjunction</a></span></li><li><span><a href=\"#Try-other-clustering-algorithms\" data-toc-modified-id=\"Try-other-clustering-algorithms-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Try other clustering algorithms</a></span></li><li><span><a href=\"#Add-data-about-the-companies(webpage-information/traffic,-finances?)\" data-toc-modified-id=\"Add-data-about-the-companies(webpage-information/traffic,-finances?)-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Add data about the companies(webpage information/traffic, finances?)</a></span></li><li><span><a href=\"#Add-textual-contextuals\" data-toc-modified-id=\"Add-textual-contextuals-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Add textual contextuals</a></span></li><li><span><a href=\"#Big-goal:-Get-.9-f1-on-Bayes-classifier(some-tuning-is-possible,-different-types-of-Bayes)\" data-toc-modified-id=\"Big-goal:-Get-.9-f1-on-Bayes-classifier(some-tuning-is-possible,-different-types-of-Bayes)-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Big goal: Get .9 f1 on Bayes classifier(some tuning is possible, different types of Bayes)</a></span></li></ul></li><li><span><a href=\"#Libraries,-initial-data-loading\" data-toc-modified-id=\"Libraries,-initial-data-loading-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Libraries, initial data loading</a></span></li><li><span><a href=\"#Beginning-to-look-at-the-data-and-deal-with-nulls\" data-toc-modified-id=\"Beginning-to-look-at-the-data-and-deal-with-nulls-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Beginning to look at the data and deal with nulls</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-lot-of-seemingly-linked-nulls-around-8.9%,-dropping-rows-of-one-of-those-columns\" data-toc-modified-id=\"A-lot-of-seemingly-linked-nulls-around-8.9%,-dropping-rows-of-one-of-those-columns-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>A lot of seemingly linked nulls around 8.9%, dropping rows of one of those columns</a></span></li><li><span><a href=\"#Dropping-38%-null-state_code\" data-toc-modified-id=\"Dropping-38%-null-state_code-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dropping 38% null state_code</a></span></li><li><span><a href=\"#A-lot-less-nulls-but-still-leaves-plenty-of-rows-and-features!-lets-examine-some-distributions\" data-toc-modified-id=\"A-lot-less-nulls-but-still-leaves-plenty-of-rows-and-features!-lets-examine-some-distributions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>A lot less nulls but still leaves plenty of rows and features! lets examine some distributions</a></span></li><li><span><a href=\"#Renaming-these-odd-columns-by-stripping-whitespace\" data-toc-modified-id=\"Renaming-these-odd-columns-by-stripping-whitespace-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Renaming these odd columns by stripping whitespace</a></span></li><li><span><a href=\"#Dropping-for-lack-of-information\" data-toc-modified-id=\"Dropping-for-lack-of-information-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Dropping for lack of information</a></span></li><li><span><a href=\"#examining-objects-first-as-all-the-floats-are-non-null\" data-toc-modified-id=\"examining-objects-first-as-all-the-floats-are-non-null-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>examining objects first as all the floats are non null</a></span></li><li><span><a href=\"#Way-many-categories-to-engineer:\" data-toc-modified-id=\"Way-many-categories-to-engineer:-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Way many categories to engineer:</a></span></li><li><span><a href=\"#Seems-to-be-an-inflated-version-of-market-in-that-market-takes-the-first-value\" data-toc-modified-id=\"Seems-to-be-an-inflated-version-of-market-in-that-market-takes-the-first-value-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Seems to be an inflated version of market in that market takes the first value</a></span></li><li><span><a href=\"#Dropping-inflated-category_list-for-now,-to-engineer-later\" data-toc-modified-id=\"Dropping-inflated-category_list-for-now,-to-engineer-later-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Dropping inflated category_list for now, to engineer later</a></span></li><li><span><a href=\"#Funding-USD-has-commas-and---as-the-most-common-value,-these-are-issues.\" data-toc-modified-id=\"Funding-USD-has-commas-and---as-the-most-common-value,-these-are-issues.-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Funding USD has commas and - as the most common value, these are issues.</a></span></li><li><span><a href=\"#This-will-be-our-target-variable:-the-status-of-a-startup,-we-should-drop-the-NaN\" data-toc-modified-id=\"This-will-be-our-target-variable:-the-status-of-a-startup,-we-should-drop-the-NaN-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>This will be our target variable: the status of a startup, we should drop the NaN</a></span></li><li><span><a href=\"#A-large-amount-of-0s-in-these-columns,-probably-drop,-but-they-might-contain-information?\" data-toc-modified-id=\"A-large-amount-of-0s-in-these-columns,-probably-drop,-but-they-might-contain-information?-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;</span>A large amount of 0s in these columns, probably drop, but they might contain information?</a></span></li><li><span><a href=\"#Geo-vars-starting-with-country-code---to-be-count-and-frequency-encoded-or-one-hot-encoded-and-then-sparse-values-dropped\" data-toc-modified-id=\"Geo-vars-starting-with-country-code---to-be-count-and-frequency-encoded-or-one-hot-encoded-and-then-sparse-values-dropped-3.13\"><span class=\"toc-item-num\">3.13&nbsp;&nbsp;</span>Geo vars starting with country code - to be count and frequency encoded or one hot encoded and then sparse values dropped</a></span></li><li><span><a href=\"#Dummy-variable-and-drop-the-rarely-occurring-regions,-probably-highly-correlated-with-country\" data-toc-modified-id=\"Dummy-variable-and-drop-the-rarely-occurring-regions,-probably-highly-correlated-with-country-3.14\"><span class=\"toc-item-num\">3.14&nbsp;&nbsp;</span>Dummy variable and drop the rarely occurring regions, probably highly correlated with country</a></span></li><li><span><a href=\"#Dropping-city-due-to-presumable-high-correlation,-or-should-it-be-the-other-way-around?\" data-toc-modified-id=\"Dropping-city-due-to-presumable-high-correlation,-or-should-it-be-the-other-way-around?-3.15\"><span class=\"toc-item-num\">3.15&nbsp;&nbsp;</span>Dropping city due to presumable high correlation, or should it be the other way around?</a></span></li></ul></li><li><span><a href=\"#Datetime-variables\" data-toc-modified-id=\"Datetime-variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Datetime variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resampling-datetime-to-year,-only-keeping-founded_at-of-year-month-quarter\" data-toc-modified-id=\"Resampling-datetime-to-year,-only-keeping-founded_at-of-year-month-quarter-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Resampling datetime to year, only keeping founded_at of year month quarter</a></span></li><li><span><a href=\"#1962-is-a-very-early-year...make-note-to-remove-early-years-when-preprocessing-datetimes\" data-toc-modified-id=\"1962-is-a-very-early-year...make-note-to-remove-early-years-when-preprocessing-datetimes-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>1962 is a very early year...make note to remove early years when preprocessing datetimes</a></span></li><li><span><a href=\"#Extracting-just-the-year-for-easier-encoding\" data-toc-modified-id=\"Extracting-just-the-year-for-easier-encoding-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Extracting just the year for easier encoding</a></span></li><li><span><a href=\"#Clearly-there-are-some-too-early-years-here\" data-toc-modified-id=\"Clearly-there-are-some-too-early-years-here-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Clearly there are some too early years here</a></span></li></ul></li><li><span><a href=\"#Is-the-year-founded-column-replaced-by-this?\" data-toc-modified-id=\"Is-the-year-founded-column-replaced-by-this?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Is the year founded column replaced by this?</a></span><ul class=\"toc-item\"><li><span><a href=\"#To-do---outliers-to-be-recoded\" data-toc-modified-id=\"To-do---outliers-to-be-recoded-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>To do - outliers to be recoded</a></span></li><li><span><a href=\"#The-vast-majority-only-make-it-to-round-1,-does-this-mean-those---in-funding-total-are-really-0?\" data-toc-modified-id=\"The-vast-majority-only-make-it-to-round-1,-does-this-mean-those---in-funding-total-are-really-0?-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>The vast majority only make it to round 1, does this mean those - in funding total are really 0?</a></span></li></ul></li><li><span><a href=\"#TO-DO:-Viz\" data-toc-modified-id=\"TO-DO:-Viz-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TO DO: Viz</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusions:\" data-toc-modified-id=\"Conclusions:-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Conclusions:</a></span></li><li><span><a href=\"#To-do:-Correlation-with-categorical-variables-encoded\" data-toc-modified-id=\"To-do:-Correlation-with-categorical-variables-encoded-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>To do: Correlation with categorical variables encoded</a></span></li><li><span><a href=\"#Trying-to-figure-out-how-to-just-get-dummies-of-the-top-ten(that's-a-google-away)\" data-toc-modified-id=\"Trying-to-figure-out-how-to-just-get-dummies-of-the-top-ten(that's-a-google-away)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Trying to figure out how to just get dummies of the top ten(that's a google away)</a></span></li><li><span><a href=\"#Encoding-and-dropping-base-variable-of-region\" data-toc-modified-id=\"Encoding-and-dropping-base-variable-of-region-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Encoding and dropping base variable of region</a></span></li><li><span><a href=\"#Dropping-over-99%-0s---is-this-good-practice?-Do-the-0s-contain-info?\" data-toc-modified-id=\"Dropping-over-99%-0s---is-this-good-practice?-Do-the-0s-contain-info?-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Dropping over 99% 0s - is this good practice? Do the 0s contain info?</a></span></li><li><span><a href=\"#Dropping-last-NaN-as-there's-very-little\" data-toc-modified-id=\"Dropping-last-NaN-as-there's-very-little-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Dropping last NaN as there's very little</a></span></li><li><span><a href=\"#Total-funding:-Gotta-impute-those---first-before-train-test-split!!!---imputing-as-0-on-funding_total_usd,-or-should-I-drop?\" data-toc-modified-id=\"Total-funding:-Gotta-impute-those---first-before-train-test-split!!!---imputing-as-0-on-funding_total_usd,-or-should-I-drop?-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Total funding: Gotta impute those - first before train test split!!! - imputing as 0 on funding_total_usd, or should I drop?</a></span></li><li><span><a href=\"#Most-likely---is-0\" data-toc-modified-id=\"Most-likely---is-0-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Most likely - is 0</a></span></li><li><span><a href=\"#A-bunch-of-thematic-columns,-why-not\" data-toc-modified-id=\"A-bunch-of-thematic-columns,-why-not-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>A bunch of thematic columns, why not</a></span></li><li><span><a href=\"#Could-make-more!-Also-strip-the-whitespace-from-columns-again.\" data-toc-modified-id=\"Could-make-more!-Also-strip-the-whitespace-from-columns-again.-6.10\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Could make more! Also strip the whitespace from columns again.</a></span></li><li><span><a href=\"#One-last-check-of-correlation\" data-toc-modified-id=\"One-last-check-of-correlation-6.11\"><span class=\"toc-item-num\">6.11&nbsp;&nbsp;</span>One last check of correlation</a></span></li></ul></li><li><span><a href=\"#Feature-Selection-pt.-1\" data-toc-modified-id=\"Feature-Selection-pt.-1-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Feature Selection pt. 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding-target-variable\" data-toc-modified-id=\"Encoding-target-variable-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Encoding target variable</a></span></li></ul></li><li><span><a href=\"#Synthetic-data/noise-generation-/-dimensional-manipulations\" data-toc-modified-id=\"Synthetic-data/noise-generation-/-dimensional-manipulations-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Synthetic data/noise generation / dimensional manipulations</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Polynomial-features---try-later-with-reduced-features-and-or-PCA!\" data-toc-modified-id=\"Polynomial-features---try-later-with-reduced-features-and-or-PCA!-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Polynomial features - try later with reduced features and or PCA!</a></span></li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Train test split</a></span></li></ul></li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayes-is-bae---and-performs-very-poorly-with-lots-of-features\" data-toc-modified-id=\"Bayes-is-bae---and-performs-very-poorly-with-lots-of-features-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Bayes is bae - and performs very poorly with lots of features</a></span></li><li><span><a href=\"#Metrics:\" data-toc-modified-id=\"Metrics:-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Metrics:</a></span></li><li><span><a href=\"#This-shows-the-very-imbalanced-classes---1-is-predominant-while-0-and-2-occupy-a-small-set-of-y\" data-toc-modified-id=\"This-shows-the-very-imbalanced-classes---1-is-predominant-while-0-and-2-occupy-a-small-set-of-y-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>This shows the very imbalanced classes - 1 is predominant while 0 and 2 occupy a small set of y</a></span></li><li><span><a href=\"#ADD-yellowbrick-classification-report\" data-toc-modified-id=\"ADD-yellowbrick-classification-report-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>ADD yellowbrick classification report</a></span></li></ul></li><li><span><a href=\"#PCA---yes-or-no?\" data-toc-modified-id=\"PCA---yes-or-no?-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>PCA - yes or no?</a></span><ul class=\"toc-item\"><li><span><a href=\"#SMOTE\" data-toc-modified-id=\"SMOTE-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>SMOTE</a></span></li><li><span><a href=\"#Bayes-performs-much-better-on-the-weighted-f1-with-SMOTE,-as-is-the-idea.\" data-toc-modified-id=\"Bayes-performs-much-better-on-the-weighted-f1-with-SMOTE,-as-is-the-idea.-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Bayes performs much better on the weighted f1 with SMOTE, as is the idea.</a></span></li><li><span><a href=\"#Oddly-goes-down-after-date-encoding-and-SMOTE---good-place-for-a-classification-report\" data-toc-modified-id=\"Oddly-goes-down-after-date-encoding-and-SMOTE---good-place-for-a-classification-report-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Oddly goes down after date encoding and SMOTE - good place for a classification report</a></span></li></ul></li><li><span><a href=\"#Feature-selection---using-most-important-features-of-lightgbm---should-this-be-done-before-or-after-SMOTE?\" data-toc-modified-id=\"Feature-selection---using-most-important-features-of-lightgbm---should-this-be-done-before-or-after-SMOTE?-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Feature selection - using most important features of lightgbm - should this be done before or after SMOTE?</a></span><ul class=\"toc-item\"><li><span><a href=\"#New-X-and-train-test-split-with-SMOTE-reduced-feature-set\" data-toc-modified-id=\"New-X-and-train-test-split-with-SMOTE-reduced-feature-set-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>New X and train test split with SMOTE reduced feature set</a></span></li><li><span><a href=\"#Trees-perform-better-thanks-to-trees\" data-toc-modified-id=\"Trees-perform-better-thanks-to-trees-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Trees perform better thanks to trees</a></span></li></ul></li><li><span><a href=\"#Clustering-as-feeders---need-to-rewrite-most-of-this-to:\" data-toc-modified-id=\"Clustering-as-feeders---need-to-rewrite-most-of-this-to:-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Clustering as feeders - need to rewrite most of this to:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Most-importantly,-add-'km_cluster'-to-existing-dataframe\" data-toc-modified-id=\"Most-importantly,-add-'km_cluster'-to-existing-dataframe-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;</span>Most importantly, add 'km_cluster' to existing dataframe</a></span></li><li><span><a href=\"#Credit-to:\" data-toc-modified-id=\"Credit-to:-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;</span>Credit to:</a></span></li><li><span><a href=\"#Slight-jump-in-f1\" data-toc-modified-id=\"Slight-jump-in-f1-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;</span>Slight jump in f1</a></span></li><li><span><a href=\"#Not-much-on-results,-trying-HDBSCAN-and-then-DBSCAN-and-t-SNE\" data-toc-modified-id=\"Not-much-on-results,-trying-HDBSCAN-and-then-DBSCAN-and-t-SNE-13.4\"><span class=\"toc-item-num\">13.4&nbsp;&nbsp;</span>Not much on results, trying HDBSCAN and then DBSCAN and t-SNE</a></span></li><li><span><a href=\"#Hdbscan's-params-should-be-examined-and-tuned-to-see-if-it-gives-results\" data-toc-modified-id=\"Hdbscan's-params-should-be-examined-and-tuned-to-see-if-it-gives-results-13.5\"><span class=\"toc-item-num\">13.5&nbsp;&nbsp;</span>Hdbscan's params should be examined and tuned to see if it gives results</a></span></li></ul></li><li><span><a href=\"#Probably-good-to-visualize-these\" data-toc-modified-id=\"Probably-good-to-visualize-these-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Probably good to visualize these</a></span></li><li><span><a href=\"#Genetic-algorithms-and-bayesian-methods-for-optimization/feature-selection-or-reduction\" data-toc-modified-id=\"Genetic-algorithms-and-bayesian-methods-for-optimization/feature-selection-or-reduction-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Genetic algorithms and bayesian methods for optimization/feature selection or reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#DBSCAN---also-needs-to-be-tuned,-doesn't-have-a-predictor-method-here\" data-toc-modified-id=\"DBSCAN---also-needs-to-be-tuned,-doesn't-have-a-predictor-method-here-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>DBSCAN - also needs to be tuned, doesn't have a predictor method here</a></span></li></ul></li><li><span><a href=\"#Making-blobs-and-junk-because-I-can't-make-this-classifier-more-accurate-after-feature-selection:-should-tune-the-clustering-algorithms-more\" data-toc-modified-id=\"Making-blobs-and-junk-because-I-can't-make-this-classifier-more-accurate-after-feature-selection:-should-tune-the-clustering-algorithms-more-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Making blobs and junk because I can't make this classifier more accurate after feature selection: should tune the clustering algorithms more</a></span></li><li><span><a href=\"#Probably-add-some-feature-importance-plots-and-other-metrics\" data-toc-modified-id=\"Probably-add-some-feature-importance-plots-and-other-metrics-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Probably add some feature importance plots and other metrics</a></span></li><li><span><a href=\"#Code-below-not-being-used\" data-toc-modified-id=\"Code-below-not-being-used-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Code below not being used</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "- ## Think about the target variable and other possible ones\n",
    "- ## Add more charts instead of value_counts(), and a function to replace that and add on\n",
    "- ## More feature engineering - mostly around outliers/the - in the funding total?\n",
    "- ## Summary statistics, probably in that function above, investigate the data deeper\n",
    "- ## Research how the funding process works(domain knowledge)\n",
    "- ## Work on tuning the clustering algorithms and using them in conjunction\n",
    "- ## Try other clustering algorithms\n",
    "- ## Add data about the companies(webpage information/traffic, finances?)\n",
    "- ## Add textual contextuals\n",
    "- ## Big goal: Get .9 f1 on Bayes classifier(some tuning is possible, different types of Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries, initial data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('investments_VC.csv', 'r', newline='', encoding='ISO-8859-1') as csvfile:\n",
    "    df = pd.read_csv(csvfile)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning to look at the data and deal with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perct_missing_values = df.isnull().sum()*100 / len(df)\n",
    "perct_missing_values.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot of seemingly linked nulls around 8.9%, dropping rows of one of those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['name'].notna()]\n",
    "\n",
    "perct_missing_values = df.isnull().sum()*100 / len(df)\n",
    "perct_missing_values.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping 38% null state_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('state_code', axis=1,inplace=True)\n",
    "\n",
    "df = df[df['founded_year'].notna()]\n",
    "\n",
    "perct_missing_values = df.isnull().sum()*100 / len(df)\n",
    "perct_missing_values.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot less nulls but still leaves plenty of rows and features! lets examine some distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming these odd columns by stripping whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping for lack of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(['permalink', 'homepage_url', 'name'], axis=1, inplace= True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examining objects first as all the floats are non null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way many categories to engineer:\n",
    "- Split on pipes / combine\n",
    "- Identify top categories\n",
    "- Does the order matter? Generally it's in market the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category_list.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems to be an inflated version of market in that market takes the first value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.market.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping inflated category_list for now, to engineer later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('category_list',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=\"market\",data=df, palette='Pastel1',\n",
    "              order=df.market.value_counts().iloc[:10].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding USD has commas and - as the most common value, these are issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['funding_total_usd'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_total_usd'] = df['funding_total_usd'].replace(',','', regex=True)\n",
    "df['funding_total_usd'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be our target variable: the status of a startup, we should drop the NaN \n",
    "- ### Are 'operating' new startups information to deal with? Are we looking at the long term?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the target variable, we can spare some dropped NAN\n",
    "df.status.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df['status'].notna()]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A large amount of 0s in these columns, probably drop, but they might contain information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.round_A.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.angel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.venture.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.equity_crowdfunding.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.undisclosed.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo vars starting with country code - to be count and frequency encoded or one hot encoded and then sparse values dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country_code'].value_counts().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variable and drop the rarely occurring regions, probably highly correlated with country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region'].value_counts().nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['city'].value_counts().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping city due to presumable high correlation, or should it be the other way around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('city',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling datetime to year, only keeping founded_at of year month quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cols = ['founded_at', 'first_funding_at', 'last_funding_at']\n",
    "\n",
    "df = df.drop(['founded_month', 'founded_quarter', 'founded_year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[dt_cols] = df[dt_cols].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1962 is a very early year...make note to remove early years when preprocessing datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.founded_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first_funding_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.last_funding_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting just the year for easier encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['founded_at'] = df.founded_at.dt.year\n",
    "df.first_funding_at = df.first_funding_at.dt.year\n",
    "df.last_funding_at = df.last_funding_at.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.founded_at.value_counts().sort_index()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearly there are some too early years here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['founded_at'] > 1980]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.founded_at.value_counts().sort_index()[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['founded_at'] > 1998] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.founded_at.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_diff'] = (df['last_funding_at'] - df['first_funding_at'])\n",
    "\n",
    "df['funding_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['founding_funding'] = (df['last_funding_at'] - df['founded_at'])\n",
    "\n",
    "df['founding_funding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['founding_funding'] = df['founding_funding'].where(df['founding_funding'] > 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the year founded column replaced by this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['years_operating'] = (2014 - df['founded_at'])\n",
    "\n",
    "df['years_operating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['founded_at', 'first_funding_at', 'last_funding_at']\n",
    "df = pd.concat([df, pd.get_dummies(df[date_cols].astype(str), drop_first=True)], axis=1)\n",
    "df = df.drop(date_cols,axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do - outliers to be recoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vast majority only make it to round 1, does this mean those - in funding total are really 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['funding_rounds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TO DO: Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "- Funding rounds are highly correlated with each other, amount of rounds with venture interest\n",
    "- Seed with project crowd funding\n",
    "- Post ipo equity brings post ipo debt\n",
    "- Secondary market is in Round G? Whatever that means.\n",
    "- Venture with all the rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: Correlation with categorical variables encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_corr(df):\n",
    "    '''Adapted from the sns documentation'''\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.cubehelix_palette(start=2.8, rot=.1, as_cmap=True)\n",
    "    # sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to figure out how to just get dummies of the top ten(that's a google away)\n",
    "- ### But for now, going to just end up dropping those with a ton of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['market'], drop_first=True)], axis=1)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('market',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['country_code'], drop_first=True)], axis=1)\n",
    "df.drop('country_code',axis=1,inplace=True)\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and dropping base variable of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['region'], drop_first=True)], axis=1)\n",
    "df.drop('region',axis=1,inplace=True)\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perct_missing_values = df.isnull().sum()*100 / len(df)\n",
    "perct_missing_values.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping over 99% 0s - is this good practice? Do the 0s contain info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, df.eq(0).mean().le(.99)]\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perct_missing_values = df.isnull().sum()*100 / len(df)\n",
    "perct_missing_values.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping last NaN as there's very little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total funding: Gotta impute those - first before train test split!!! - imputing as 0 on funding_total_usd, or should I drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_total_usd'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most likely - is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['funding_total_usd'].replace(' -   ',0, inplace=True)\n",
    "\n",
    "df['funding_total_usd'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_total_usd']= df['funding_total_usd'].str.strip()\n",
    "\n",
    "df.funding_total_usd.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_total_usd']= df['funding_total_usd'].fillna(0)\n",
    "\n",
    "df.funding_total_usd.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funding_total_usd'] = df['funding_total_usd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['per_round'] = df['funding_total_usd'] / df['funding_rounds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bunch of thematic columns, why not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could make more! Also strip the whitespace from columns again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['East_Coast'] = df['Boston'] + df[\"New York City\"]\n",
    "# df['All_Software'] = df[' Enterprise Software '] + df[' Hardware + Software '] + df[' Software ']\n",
    "# df['Mobile_Games'] = df[' Mobile '] + df[' Games ']\n",
    "# df['Online_Business'] = df[' Advertising '] + df[' E-Commerce ']\n",
    "df['funding_types'] = df['seed'] + df['venture'] + df['debt_financing'] + df['angel'] + df['private_equity']\n",
    "df['funding_minus_start'] = df['funding_total_usd'] - df['founding_funding']\n",
    "df['funding_minus_startA'] = df['funding_total_usd'] - df['round_A']\n",
    "df['funding_minus_start_seed'] = df['funding_total_usd'] - df['seed']\n",
    "df['funding_plus_start'] = df['funding_total_usd'] + df['founding_funding']\n",
    "df['funding_plus_startA'] = df['funding_total_usd'] + df['round_A']\n",
    "df['funding_plus_start_seed'] = df['funding_total_usd'] + df['seed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One last check of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection pt. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'status':{'closed':0,'acquired':2,'operating':1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('status',axis=1)\n",
    "y = df['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data/noise generation / dimensional manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_x = df2.drop('status',axis=1).applymap(lambda x: x + np.random.normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_x['status'] = np.random.permutation(df2['status'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_x['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rand_x[rand_x < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = rand_x\n",
    "# y = rand_x['status']\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# model = SVC()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "\n",
    "# clf = xgb.XGBClassifier(max_depth=5, objective='multi:softmax', n_estimators=100, \n",
    "#                         num_classes=3)\n",
    "\n",
    "# clf.fit(X_train, y_train)  \n",
    "\n",
    "# y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas_montecarlo\n",
    "# mc = df['status'].montecarlo(sims=10, bust=-0.1, goal=1)\n",
    "\n",
    "# mc.plot(title=\"SPY Returns Monte Carlo Simulations\")  # optional: , figsize=(x, y)\n",
    "# # Show test stats\n",
    "\n",
    "# print(mc.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Show bust / max drawdown stats\n",
    "\n",
    "# print(mc.maxdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mc.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features - try later with reduced features and or PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#Finding features with 99% of the same value\n",
    "no_var = VarianceThreshold(threshold=0.01)\n",
    "no_var.fit(X_train)\n",
    "print([x for x in X_train.columns if x not in X_train.columns[no_var.get_support()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes is bae - and performs very poorly with lots of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes too big tree\n",
    "# from graphviz import Source\n",
    "# Source( tree.export_graphviz(clf, out_file=None, feature_names=X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: \n",
    "- Weighted f1 \n",
    "- Brier loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "visualizer = ClassificationReport(clf, classes=clf.classes_, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)       \n",
    "visualizer.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "viz = PrecisionRecallCurve(clf)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = PrecisionRecallCurve(\n",
    "    clf, per_class=True, iso_f1_curves=True,\n",
    "    fill_area=False, micro=False, classes=clf.classes_\n",
    ")\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This shows the very imbalanced classes - 1 is predominant while 0 and 2 occupy a small set of y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "\n",
    "visualizer = ClassBalance(labels=clf.classes_)\n",
    "\n",
    "visualizer.fit(y_train, y_test)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - yes or no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn import datasets\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# # Define a pipeline to search for the best combination of PCA truncation\n",
    "# # and classifier regularization.\n",
    "# pca = PCA()\n",
    "# # set the tolerance to a large value to make the example faster\n",
    "# logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "# X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "# param_grid = {\n",
    "#     'pca__n_components': [5, 15, 30, 45, 64],\n",
    "#     'logistic__C': np.logspace(-4, 4, 4),\n",
    "# }\n",
    "# search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "# search.fit(X_digits, y_digits)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)\n",
    "\n",
    "# # Plot the PCA spectrum\n",
    "# pca.fit(X_digits)\n",
    "\n",
    "# fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "# ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "#          pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "# ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "# ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "#             linestyle=':', label='n_components chosen')\n",
    "# ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# # For each number of components, find the best classifier results\n",
    "# results = pd.DataFrame(search.cv_results_)\n",
    "# components_col = 'param_pca__n_components'\n",
    "# best_clfs = results.groupby(components_col).apply(\n",
    "#     lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "# best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "#                legend=False, ax=ax1)\n",
    "# ax1.set_ylabel('Classification accuracy (val)')\n",
    "# ax1.set_xlabel('n_components')\n",
    "\n",
    "# plt.xlim(-1, 70)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes performs much better on the weighted f1 with SMOTE, as is the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oddly goes down after date encoding and SMOTE - good place for a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection - using most important features of lightgbm - should this be done before or after SMOTE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, max_features=9)\n",
    "embeded_lgb_selector.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lgb_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New X and train test split with SMOTE reduced feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[embeded_lgb_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees perform better thanks to trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering as feeders - need to rewrite most of this to:\n",
    "- Better params for feeder function \n",
    "- Elbow method abstraction\n",
    "## Most importantly, add 'km_cluster' to existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_train)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit to:\n",
    "- https://github.com/mudassirkhan19/cluster-classification/blob/master/Kmeans_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class clust():\n",
    "    def _load_data(self, sklearn_load_ds ):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "    def __init__(self, sklearn_load_ds):\n",
    "        self._load_data(sklearn_load_ds)\n",
    "    \n",
    "# =xgb.XGBClassifier(max_depth=5, objective='multi:softmax', n_estimators=100, \n",
    "#                         num_classes=3)):\n",
    "    def classify(self, model):\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        print('f1: {}'.format(f1_score(self.y_test, y_pred, average='weighted')))\n",
    "\n",
    "\n",
    "    def Kmeans(self, output='add'):\n",
    "        n_clusters = 3\n",
    "        len(np.unique(self.y_train))\n",
    "        clf = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        clf.fit(self.X_train)\n",
    "        y_labels_train = clf.labels_\n",
    "        y_labels_test = clf.predict(self.X_test)\n",
    "        if output == 'add':\n",
    "            self.X_train['km_clust'] = y_labels_train\n",
    "            self.X_test['km_clust'] = y_labels_test\n",
    "        elif output == 'replace':\n",
    "            self.X_train = y_labels_train[:, np.newaxis]\n",
    "            self.X_test = y_labels_test[:, np.newaxis]\n",
    "        else:\n",
    "            raise ValueError('output should be either add or replace')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slight jump in f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clust(df).Kmeans(output='add').classify(model=DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not much on results, trying HDBSCAN and then DBSCAN and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hdbscan's params should be examined and tuned to see if it gives results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, prediction_data=True).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['cluster'] = clusterer.labels_\n",
    "X_test['cluster'] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probably good to visualize these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette('Paired', 12)\n",
    "cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer.probabilities_)]\n",
    "plt.scatter(*projection.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic algorithms and bayesian methods for optimization/feature selection or reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/feature-reduction-using-genetic-algorithm-with-python-403a5f4ef0c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import sklearn.svm\n",
    "\n",
    "\n",
    "# def reduce_features(solution, features):\n",
    "#     '''Gets the reduced features'''\n",
    "#     # gets the indices of solution = 1, or to keep in the model\n",
    "#     selected_elements_indices = numpy.where(solution == 1)[0]\n",
    "    \n",
    "#     #the reduced features derived from the features, only at the selected indices of solution = 1 \n",
    "#     reduced_features = features[:, selected_elements_indices]\n",
    "#     return reduced_features\n",
    "\n",
    "\n",
    "# def classification_accuracy(labels, predictions):\n",
    "    \n",
    "#     #correct where labels are predictions, accuracy is the % of correct ones\n",
    "#     correct = numpy.where(labels == predictions)[0]\n",
    "#     accuracy = correct.shape[0]/labels.shape[0]\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# def cal_pop_fitness(pop, features, labels, train_indices, test_indices):\n",
    "#     '''Calculates population fitness\n",
    "#     First, accuracies = 0 multidimensional array of population shape \n",
    "#     Idx = 0, starting index counter \n",
    "#     Goes through population of solutions \n",
    "#     Reduced features = calls reduce_features, getting the indices that have solution 1 by passing in the current solution,\n",
    "#     and giving the features to compare indices to\n",
    "#     classifier is invoked and fitted \n",
    "#     predictions are gathered and accuracy calculated (this could become a paramater of what to base accuracy on,\n",
    "#     I would want weighted f1)\n",
    "#     Go through population of solutions fully\n",
    "#     Return accuracies \n",
    "#     Called in \n",
    "#     #for each generation\n",
    "# for generation in range(num_generations):\n",
    "#     print(\"Generation : \", generation)\n",
    "#     # Measuring the fitness of each chromosome in the population.\n",
    "#     fitness = GA.cal_pop_fitness(new_population, data_inputs, data_outputs, train_indices, test_indices)\n",
    "\n",
    "#     '''\n",
    "#     accuracies = numpy.zeros(pop.shape[0])\n",
    "#     idx = 0\n",
    "\n",
    "#     for curr_solution in pop:\n",
    "#         reduced_features = reduce_features(curr_solution, features)\n",
    "#         train_data = reduced_features[train_indices, :]\n",
    "#         test_data = reduced_features[test_indices, :]\n",
    "\n",
    "#         train_labels = labels[train_indices]\n",
    "#         test_labels = labels[test_indices]\n",
    "\n",
    "#         SV_classifier = sklearn.svm.SVC(gamma='scale')\n",
    "#         SV_classifier.fit(X=train_data, y=train_labels)\n",
    "\n",
    "#         predictions = SV_classifier.predict(test_data)\n",
    "#         accuracies[idx] = classification_accuracy(test_labels, predictions)\n",
    "#         idx = idx + 1\n",
    "#     return accuracies\n",
    "\n",
    "# def select_mating_pool(pop, fitness, num_parents):\n",
    "#     # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n",
    "#     parents = numpy.empty((num_parents, pop.shape[1]))\n",
    "#     for parent_num in range(num_parents):\n",
    "#         max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "#         max_fitness_idx = max_fitness_idx[0][0]\n",
    "#         parents[parent_num, :] = pop[max_fitness_idx, :]\n",
    "#         fitness[max_fitness_idx] = -99999999999\n",
    "#     return parents\n",
    "\n",
    "\n",
    "# def crossover(parents, offspring_size):\n",
    "#     offspring = numpy.empty(offspring_size)\n",
    "#     # The point at which crossover takes place between two parents. Usually, it is at the center.\n",
    "#     crossover_point = numpy.uint8(offspring_size[1]/2)\n",
    "\n",
    "#     for k in range(offspring_size[0]):\n",
    "#         # Index of the first parent to mate.\n",
    "#         parent1_idx = k%parents.shape[0]\n",
    "#         # Index of the second parent to mate.\n",
    "#         parent2_idx = (k+1)%parents.shape[0]\n",
    "#         # The new offspring will have its first half of its genes taken from the first parent.\n",
    "#         offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "#         # The new offspring will have its second half of its genes taken from the second parent.\n",
    "#         offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "#     return offspring\n",
    "\n",
    "\n",
    "# def mutation(offspring_crossover, num_mutations=2):\n",
    "#     mutation_idx = numpy.random.randint(low=0, high=offspring_crossover.shape[1], size=num_mutations)\n",
    "#     # Mutation changes a single gene in each offspring randomly.\n",
    "#     for idx in range(offspring_crossover.shape[0]):\n",
    "#         # The random value to be added to the gene.\n",
    "#         offspring_crossover[idx, mutation_idx] = 1 - offspring_crossover[idx, mutation_idx]\n",
    "#     return offspring_crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = data_inputs.shape[0]\n",
    "# num_feature_elements = data_inputs.shape[1]\n",
    "# train_indices = numpy.arange(1, num_samples, 4)\n",
    "# test_indices = numpy.arange(0, num_samples, 4)\n",
    "# print(\"Number of training samples: \", train_indices.shape[0])\n",
    "# print(\"Number of test samples: \", test_indices.shape[0])\n",
    "\n",
    "# \"\"\"\n",
    "# Genetic algorithm parameters:\n",
    "#     Population size\n",
    "#     Mating pool size\n",
    "#     Number of mutations\n",
    "# \"\"\"\n",
    "\n",
    "# sol_per_pop = 8 # Population size.\n",
    "# num_parents_mating = 4 # Number of parents inside the mating pool.\n",
    "# num_mutations = 3 # Number of elements to mutate.\n",
    "# # Defining the population shape.\n",
    "# pop_shape = (sol_per_pop, num_feature_elements)\n",
    " \n",
    "# # Creating the initial population.\n",
    "# new_population = numpy.random.randint(low=0, high=2, size=pop_shape)\n",
    "# print(new_population.shape)\n",
    " \n",
    "# #holds the best solutions after each generation\n",
    "# best_outputs = []\n",
    "\n",
    "# num_generations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for each generation\n",
    "# for generation in range(num_generations):\n",
    "#     print(\"Generation : \", generation)\n",
    "#     # Measuring the fitness of each chromosome in the population.\n",
    "#     fitness = GA.cal_pop_fitness(new_population, data_inputs, data_outputs, train_indices, test_indices)\n",
    "\n",
    "#     #add the best (max numpy) to the best_outputs array\n",
    "#     best_outputs.append(numpy.max(fitness))\n",
    "#     # The best result in the current iteration.\n",
    "#     print(\"Best result : \", best_outputs[-1])\n",
    "\n",
    "#     # Selecting the best parents in the population for mating.\n",
    "#     parents = GA.select_mating_pool(new_population, fitness, num_parents_mating)\n",
    "\n",
    "#     # Generating next generation using crossover.\n",
    "#     offspring_crossover = GA.crossover(parents, offspring_size=(pop_shape[0]-parents.shape[0], num_feature_elements))\n",
    "\n",
    "#     # Adding some variations to the offspring using mutation.\n",
    "#     offspring_mutation = GA.mutation(offspring_crossover, num_mutations=num_mutations)\n",
    "\n",
    "#     # Creating the new population based on the parents and offspring.\n",
    "#     new_population[0:parents.shape[0], :] = parents\n",
    "#     new_population[parents.shape[0]:, :] = offspring_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitness = GA.cal_pop_fitness(new_population, data_inputs, data_outputs, train_indices, test_indices)\n",
    "\n",
    "# # Then return the index of that solution corresponding to the best fitness.\n",
    "# best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n",
    "# best_match_idx = best_match_idx[0]\n",
    "\n",
    "# best_solution = new_population[best_match_idx, :]\n",
    "# best_solution_indices = numpy.where(best_solution == 1)[0]\n",
    "# best_solution_num_elements = best_solution_indices.shape[0]\n",
    "# best_solution_fitness = fitness[best_match_idx]\n",
    "\n",
    "# print(\"best_match_idx : \", best_match_idx)\n",
    "# print(\"best_solution : \", best_solution)\n",
    "# print(\"Selected indices : \", best_solution_indices)\n",
    "# print(\"Number of selected elements : \", best_solution_num_elements)\n",
    "# print(\"Best solution fitness : \", best_solution_fitness)\n",
    "\n",
    "# matplotlib.pyplot.plot(best_outputs)\n",
    "# matplotlib.pyplot.xlabel(\"Iteration\")\n",
    "# matplotlib.pyplot.ylabel(\"Fitness\")\n",
    "# matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN - also needs to be tuned, doesn't have a predictor method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = DBSCAN(eps=0.3, min_samples=10).fit(X_train)\n",
    "# core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n",
    "# labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['dbscan'] = db.labels_\n",
    "# X_test['dbscan'] = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# clf = OneVsRestClassifier(GaussianNB())\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making blobs and junk because I can't make this classifier more accurate after feature selection: should tune the clustering algorithms more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_blobs\n",
    "# data4 = make_blobs(n_samples=37557, n_features=5, centers=3, cluster_std=2.5, \n",
    "#                    center_box=(0, 10000), shuffle=True, random_state=None)\n",
    "# df4 = pd.DataFrame(data4[0],columns=['x'+str(i) for i in range(1,6)])\n",
    "# df4['status'] = data4[1]\n",
    "\n",
    "# df4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.max(axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probably add some feature importance plots and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below not being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn import metrics\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # #############################################################################\n",
    "# # Compute DBSCAN\n",
    "# db = DBSCAN(eps=0.3, min_samples=10).fit(X_train)\n",
    "# core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "# core_samples_mask[db.core_sample_indices_] = True\n",
    "# labels = db.labels_\n",
    "\n",
    "# # Number of clusters in labels, ignoring noise if present.\n",
    "# n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "# n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# print('Estimated number of clusters: %d' % n_clusters_)\n",
    "# print('Estimated number of noise points: %d' % n_noise_)\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y_train, labels))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(y_train, labels))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(y_train, labels))\n",
    "# print(\"Adjusted Rand Index: %0.3f\"\n",
    "#       % metrics.adjusted_rand_score(y_train, labels))\n",
    "# print(\"Adjusted Mutual Information: %0.3f\"\n",
    "#       % metrics.adjusted_mutual_info_score(y_train, labels))\n",
    "# print(\"Silhouette Coefficient: %0.3f\"\n",
    "#       % metrics.silhouette_score(X_train, labels))\n",
    "\n",
    "# # #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Black removed and is used for noise instead.\n",
    "# unique_labels = set(labels)\n",
    "# colors = [plt.cm.Spectral(each)\n",
    "#           for each in np.linspace(0, 1, len(unique_labels))]\n",
    "# for k, col in zip(unique_labels, colors):\n",
    "#     if k == -1:\n",
    "#         # Black used for noise.\n",
    "#         col = [0, 0, 0, 1]\n",
    "\n",
    "#     class_member_mask = (labels == k)\n",
    "\n",
    "#     xy = X_train[class_member_mask & core_samples_mask]\n",
    "#     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "#              markeredgecolor='k', markersize=14)\n",
    "\n",
    "#     xy = X[class_member_mask & ~core_samples_mask]\n",
    "#     plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "#              markeredgecolor='k', markersize=6)\n",
    "\n",
    "# plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# data = X_train.iloc[:, 0:1].values\n",
    "\n",
    "# outlier_df = pd.DataFrame(X_train)\n",
    "\n",
    "# # Printing total number of values for each label\n",
    "# print(Counter(db.labels_))\n",
    "\n",
    "# # Printing DataFrame being considered as Outliers -1\n",
    "# print(outlier_df[db.labels_ == -1])\n",
    "\n",
    "# # Printing and Indicating which type of object outlier_df is\n",
    "# print(type(db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
